{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data into Pandas for exploration\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df = pd.read_csv(fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few entries of temperature data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_spark.write.parquet(\"sas_data\")\n",
    "df_spark=spark.read.parquet(\"sas_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "|5748522.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20579.0|  57.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1959.0|10292016|     M|  null|     NZ|9.498180283E10|00010|      B2|\n",
      "|5748523.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  66.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1950.0|10292016|     F|  null|     NZ|9.497968993E10|00010|      B2|\n",
      "|5748524.0|2016.0|   4.0| 245.0| 464.0|    HHW|20574.0|    1.0|     HI|20586.0|  41.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1975.0|10292016|     F|  null|     NZ|9.497974673E10|00010|      B2|\n",
      "|5748525.0|2016.0|   4.0| 245.0| 464.0|    HOU|20574.0|    1.0|     FL|20581.0|  27.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1989.0|10292016|     M|  null|     NZ|9.497324663E10|00028|      B2|\n",
      "|5748526.0|2016.0|   4.0| 245.0| 464.0|    LOS|20574.0|    1.0|     CA|20581.0|  26.0|    2.0|  1.0|20160430|     ACK| null|      G|      O|   null|      M| 1990.0|10292016|     F|  null|     NZ|9.501354793E10|00002|      B2|\n",
      "|5748527.0|2016.0|   4.0| 245.0| 504.0|    NEW|20574.0|    1.0|     MA|20576.0|  44.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1972.0|10292016|     M|  null|     UA|9.493828593E10|01215|      B2|\n",
      "|5748528.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|   null|20575.0|  39.0|    2.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1977.0|10292016|     M|  null|     CM|9.501810463E10|00472|      B2|\n",
      "|5748529.0|2016.0|   4.0| 245.0| 504.0|    WAS|20574.0|    1.0|     VA|20596.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10292016|     M|  null|     CM|9.492489983E10|00488|      B2|\n",
      "|5748530.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  56.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1960.0|10292016|     F|  null|     CM|9.492648103E10|00302|      B2|\n",
      "|5748531.0|2016.0|   4.0| 245.0| 504.0|    LOS|20574.0|    1.0|     CA|20577.0|  38.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1978.0|10282016|     M|  null|     CM|9.492629303E10|00302|      B2|\n",
      "|5748532.0|2016.0|   4.0| 245.0| 504.0|    MIA|20574.0|    1.0|     FL|20581.0|  53.0|    2.0|  1.0|20160430|     PNM| null|      G|      O|   null|      M| 1963.0|10292016|     F|  null|     CM|9.500640513E10|00430|      B2|\n",
      "|5748534.0|2016.0|   4.0| 245.0| 528.0|    SFR|20574.0|    1.0|     CA|   null|  84.0|    2.0|  1.0|20160430|     HNK| null|      G|   null|   null|   null| 1932.0|10282016|     F|  null|     CX|9.492476223E10|00872|      B2|\n",
      "|5748876.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  43.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1973.0|10292016|     M|  null|     UA|9.499463063E10|05574|      B1|\n",
      "|5748877.0|2016.0|   4.0| 245.0| 582.0|    HOU|20574.0|    1.0|     TX|20583.0|  30.0|    1.0|  1.0|20160430|     GUZ| null|      G|      O|   null|      M| 1986.0|10292016|     F|  null|     UA|9.499447663E10|05574|      B1|\n",
      "|5748881.0|2016.0|   4.0| 245.0| 582.0|    LOS|20574.0|    1.0|     CA|20575.0|  34.0|    2.0|  1.0|20160430|     SHG| null|      G|      O|   null|      M| 1982.0|10292016|     M|  null|     AM|9.496770903E10|00646|      B2|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|5817117.0|2016.0|   4.0| 582.0|   582|    HOU|20574.0|    1.0|      0|20575.0|  52.0|    2.0|  1.0|20160430|    null| null|      K|      O|   null|      M| 1964.0|10302016|     M|  null|     UA| 9.496104673E10|00428|      B2|\n",
      "| 460015.0|2016.0|   4.0| 135.0|   135|    ATL|20547.0|    1.0|     71|20568.0|  34.0|    2.0|  1.0|20160403|    null| null|      O|      O|   null|      M| 1982.0|07012016|  null|  null|     VS|5.5552866533E10|00109|      WT|\n",
      "| 472601.0|2016.0|   4.0| 135.0|   135|    SFR|20547.0|    1.0|     33|20553.0|  37.0|    1.0|  1.0|20160403|    null| null|      O|      O|   null|      M| 1979.0|07012016|  null|  null|     VS|5.5566831033E10|00019|      WB|\n",
      "| 636370.0|2016.0|   4.0| 111.0|   111|    CHI|20548.0|    1.0|      0|20550.0|  47.0|    1.0|  1.0|20160404|    null| null|      O|      O|   null|      M| 1969.0|07022016|  null|  null|     UA|5.5601108933E10|00986|      WB|\n",
      "| 658149.0|2016.0|   4.0| 135.0|   135|    WAS|20548.0|    1.0|      0|20552.0|  50.0|    1.0|  1.0|20160404|    null| null|      O|      O|   null|      M| 1966.0|07022016|  null|  null|     UA|5.5614471033E10|00919|      WB|\n",
      "| 671804.0|2016.0|   4.0| 148.0|   135|    ATL|20548.0|    1.0|     30|20551.0|  62.0|    1.0|  1.0|20160404|    null| null|      O|      O|   null|      M| 1954.0|07022016|  null|  null|     VS|5.5605698633E10|00109|      WB|\n",
      "| 677010.0|2016.0|   4.0| 148.0|   112|    CHI|20548.0|    1.0|      0|20552.0|  47.0|    1.0|  1.0|20160404|    null| null|      O|      O|   null|      M| 1969.0|07022016|  null|  null|     UA|5.5616218233E10|00953|      WB|\n",
      "| 687581.0|2016.0|   4.0| 213.0|   213|    NYC|20548.0|    1.0|     10|20549.0|  19.0|    2.0|  1.0|20160404|    null| null|      O|      I|   null|      M| 1997.0|10032016|  null|  null|     VS| 9.277007403E10|00025|      B2|\n",
      "| 752314.0|2016.0|   4.0| 582.0|   582|    HOU|20548.0|    1.0|      0|20551.0|  46.0|    1.0|  1.0|20160404|    null| null|      K|      O|   null|      M| 1970.0|10042016|     M|  null|     UA| 9.273432863E10|05616|      B1|\n",
      "| 822613.0|2016.0|   4.0| 111.0|   111|    NYC|20549.0|    1.0|     10|20559.0|  40.0|    2.0|  1.0|20160405|    null| null|      O|      O|   null|      M| 1976.0|07032016|  null|  null|     SN|5.5665625733E10|00501|      WT|\n",
      "| 822614.0|2016.0|   4.0| 111.0|   111|    NYC|20549.0|    1.0|     10|20559.0|   8.0|    2.0|  1.0|20160405|    null| null|      O|      O|   null|      M| 2008.0|07032016|  null|  null|     SN|5.5665624833E10|00501|      WT|\n",
      "| 829275.0|2016.0|   4.0| 124.0|   135|    TAM|20549.0|    1.0|     20|20551.0|  53.0|    1.0|  1.0|20160405|    null| null|      O|      O|   null|      M| 1963.0|07032016|  null|  null|     UA|5.5649619833E10|00123|      WB|\n",
      "|1826690.0|2016.0|   4.0| 209.0|   209|    DEN|20554.0|    1.0|      0|20560.0|  48.0|    1.0|  1.0|20160410|    null| null|      O|      Q|   null|      M| 1968.0|07082016|  null|  null|     UA|5.5925987433E10|00138|      WB|\n",
      "|1984720.0|2016.0|   4.0| 115.0|   115|    BOS|20555.0|    1.0|     10|20556.0|  30.0|    1.0|  1.0|20160411|    null| null|      O|      R|   null|      M| 1986.0|07092016|  null|  null|     WW|5.6008400333E10|0125C|      WB|\n",
      "|2001603.0|2016.0|   4.0| 135.0|   135|    WAS|20555.0|    1.0|     20|20557.0|  39.0|    1.0|  1.0|20160411|    null| null|      O|      Q|   null|      M| 1977.0|07092016|  null|  null|     UA|5.6017918033E10|01133|      WB|\n",
      "|2001890.0|2016.0|   4.0| 135.0|   135|    NYC|20555.0|    1.0|      0|20559.0|  56.0|    1.0|  1.0|20160411|    null| null|      O|      O|   null|      M| 1960.0|07092016|  null|  null|     UA|5.5990796733E10|00879|      WB|\n",
      "|2012377.0|2016.0|   4.0| 141.0|   141|    CHI|20555.0|    1.0|     63|20559.0|  37.0|    1.0|  1.0|20160411|    null| null|      O|      O|   null|      M| 1979.0|07092016|  null|  null|     UA|5.5997606633E10|00906|      WB|\n",
      "|2157981.0|2016.0|   4.0| 115.0|   115|    BOS|20556.0|    1.0|     11|20557.0|  28.0|    1.0|  1.0|20160412|    null| null|      O|      R|   null|      M| 1988.0|10112016|  null|  null|     WW| 9.336128003E10|0125C|      B1|\n",
      "|2181986.0|2016.0|   4.0| 148.0|   112|    CHI|20556.0|    1.0|      0|20565.0|  45.0|    1.0|  1.0|20160412|    null| null|      O|      O|   null|      M| 1971.0|07102016|  null|  null|     UA|5.6041863233E10|00906|      WB|\n",
      "|4499251.0|2016.0|   4.0| 131.0|   131|    WAS|20568.0|    1.0|      0|20572.0|  44.0|    1.0|  1.0|20160424|    null| null|      O|      O|   null|      M| 1972.0|07222016|  null|  null|     UA|5.9184906033E10|00975|      WB|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean I94 immigration data\n",
    "import csv\n",
    "# Create dictionary of valid i94port codes\n",
    "i94port_valid_dict = {}\n",
    "\n",
    "with open('valid_i94_port.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    i94port_valid_dict = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "    \n",
    "i94city_valid_dict = {}\n",
    "\n",
    "with open('i94_vaid_cities.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    i94city_valid_dict = {rows[0]:rows[1] for rows in reader}\n",
    "\n",
    "# Filter out entries where i94port is invalid\n",
    "df_clean_spark_i94 = df_spark.filter(df_spark.i94port.isin(list(i94port_valid_dict.keys())))\n",
    "\n",
    "\n",
    "# Filter out entries where i94res is invalid\n",
    "from pyspark.sql.types import IntegerType\n",
    "df_clean_spark_i94 = df_clean_spark_i94.withColumn(\"i94res\", df_clean_spark_i94[\"i94res\"].cast(IntegerType()))\n",
    "df_clean_spark_i94 = df_clean_spark_i94.filter(df_clean_spark_i94.i94res.isin(list(i94city_valid_dict.keys())))\n",
    "\n",
    "\n",
    "# Filter out entries where I94address is not clearly called out, as it would create anamolies in state wise analytics\n",
    "df_clean_spark_i94=df_clean_spark_i94[df_clean_spark_i94['i94addr']!=99]\n",
    "\n",
    "\n",
    "# Filter out entries where I94MODE is not reported\n",
    "df_clean_spark_i94=df_clean_spark_i94[df_clean_spark_i94['i94mode']!=9]\n",
    "\n",
    "\n",
    "# Filter out entries where I94VISA,visatype and ftlno are Null\n",
    "df_clean_spark_i94=df_clean_spark_i94.filter((df_clean_spark_i94.i94visa != 'NaN') &(df_clean_spark_i94.fltno != 'NaN')& (df_clean_spark_i94.visatype != 'NaN'))\n",
    "\n",
    "# Show dataframe\n",
    "df_clean_spark_i94.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+----+\n",
      "|i94port|          valid_city|state| _c3|\n",
      "+-------+--------------------+-----+----+\n",
      "|    ALC|               ALCAN|  AK |null|\n",
      "|    ANC|           ANCHORAGE|   AK|null|\n",
      "|    BAR|BAKER AAF - BAKER...|   AK|null|\n",
      "|    DAC|       DALTONS CACHE| AK  |null|\n",
      "|    PIZ|DEW STATION PT LA...|   AK|null|\n",
      "|    DTH|        DUTCH HARBOR|   AK|null|\n",
      "|    EGL|               EAGLE|  AK |null|\n",
      "|    FRB|           FAIRBANKS|   AK|null|\n",
      "|    HOM|               HOMER|   AK|null|\n",
      "|    HYD|               HYDER|  AK |null|\n",
      "|    JUN|              JUNEAU|   AK|null|\n",
      "|    5KE|           KETCHIKAN|   AK|null|\n",
      "|    KET|           KETCHIKAN|   AK|null|\n",
      "|    MOS|MOSES POINT INTER...|   AK|null|\n",
      "|    NIK|             NIKISKI| AK  |null|\n",
      "|    NOM|                 NOM|   AK|null|\n",
      "|    PKC|         POKER CREEK|  AK |null|\n",
      "|    ORI|      PORT LIONS SPB|   AK|null|\n",
      "|    SKA|             SKAGWAY| AK  |null|\n",
      "|    SNP|     ST. PAUL ISLAND|   AK|null|\n",
      "+-------+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt| AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|              6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01| 5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|             10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01| 14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|             16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-09-01| 12.780999999999999|                        1.454|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-10-01|               7.95|                         1.63|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-11-01|  4.638999999999999|           1.3019999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-12-01|0.12199999999999987|                        1.756|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-01-01|-1.3330000000000002|                        1.642|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-02-01|             -2.732|                        1.358|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-03-01|              0.129|                        1.088|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1745-04-01|              4.042|                        1.138|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-01-01|              1.699|                        1.013|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-02-01| 3.9610000000000003|           2.3609999999999998|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-03-01|              5.182|                         3.48|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-04-01|              7.197|                        0.732|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-05-01|             10.634|                        1.351|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-06-01|             14.913|                        1.181|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1750-07-01|             17.831|                         1.22|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+-------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean temperature data\n",
    "df_clean_spark_temp=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../../data2/GlobalLandTemperaturesByCity.csv\")\n",
    "valid_i94_port_df=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"valid_i94_port.csv\")\n",
    "valid_i94_port_df.show()\n",
    "\n",
    "# Filter out entries with NaN average temperature\n",
    "df_clean_spark_temp=df_clean_spark_temp.filter(df_clean_spark_temp.AverageTemperature != 'NaN')\n",
    "\n",
    "\n",
    "\n",
    "#df_clean_spark_temp =df_clean_spark_temp[df_clean_spark_temp['City']=='Perth']\n",
    "df_clean_spark_temp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+----------+-----+----+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    City|       Country|Latitude|Longitude|i94port|valid_city|state| _c3|\n",
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+----------+-----+----+\n",
      "|1743-11-01|             8.758|                        1.886|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-04-01|6.0699999999999985|           2.9339999999999997|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-05-01|             7.751|                        1.494|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-06-01|             10.62|                        1.574|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-07-01|             12.35|                        1.591|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-09-01|            11.224|           1.6059999999999999|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-10-01|             8.945|           1.7309999999999999|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-11-01|             7.836|                        1.585|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1744-12-01|             5.263|                        1.871|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1745-01-01|             4.136|                        1.825|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1745-02-01|             2.436|           1.6769999999999998|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1745-03-01|              3.24|                        1.456|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1745-04-01|             4.819|                        1.429|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-01-01|             5.441|                        1.463|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-02-01|              7.31|                        2.474|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-03-01|             7.335|           3.0239999999999996|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-04-01|             6.604|                        1.308|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-05-01| 8.328000000000001|           1.5090000000000001|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-06-01|            10.803|                        1.393|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "|1750-07-01|            14.367|                          1.4|aberdeen|United Kingdom|  57.05N|    1.48W|    ABE|  aberdeen|  WA |null|\n",
      "+----------+------------------+-----------------------------+--------+--------------+--------+---------+-------+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add iport94 code based on city name\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "df_clean_spark_temp = df_clean_spark_temp.withColumn(\"City\",f.lower(f.col(\"City\")))\n",
    "valid_i94_port_df = valid_i94_port_df.withColumn(\"valid_city\",f.lower(f.col(\"valid_city\")))\n",
    "\n",
    "df_clean_spark_temp=df_clean_spark_temp.join(valid_i94_port_df,valid_i94_port_df.valid_city.contains(df_clean_spark_temp.City),how=\"inner\")\n",
    "\n",
    "# Show results\n",
    "df_clean_spark_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+-------+-------+-------+\n",
      "| i94yr|i94mon|i94cit|i94port|arrdate|i94mode|depdate|i94visa|\n",
      "+------+------+------+-------+-------+-------+-------+-------+\n",
      "|2016.0|   4.0| 582.0|    HOU|20574.0|    1.0|20575.0|    2.0|\n",
      "|2016.0|   4.0| 135.0|    ATL|20547.0|    1.0|20568.0|    2.0|\n",
      "|2016.0|   4.0| 135.0|    SFR|20547.0|    1.0|20553.0|    1.0|\n",
      "|2016.0|   4.0| 111.0|    CHI|20548.0|    1.0|20550.0|    1.0|\n",
      "|2016.0|   4.0| 135.0|    WAS|20548.0|    1.0|20552.0|    1.0|\n",
      "|2016.0|   4.0| 148.0|    ATL|20548.0|    1.0|20551.0|    1.0|\n",
      "|2016.0|   4.0| 148.0|    CHI|20548.0|    1.0|20552.0|    1.0|\n",
      "|2016.0|   4.0| 213.0|    NYC|20548.0|    1.0|20549.0|    2.0|\n",
      "|2016.0|   4.0| 582.0|    HOU|20548.0|    1.0|20551.0|    1.0|\n",
      "|2016.0|   4.0| 111.0|    NYC|20549.0|    1.0|20559.0|    2.0|\n",
      "|2016.0|   4.0| 111.0|    NYC|20549.0|    1.0|20559.0|    2.0|\n",
      "|2016.0|   4.0| 124.0|    TAM|20549.0|    1.0|20551.0|    1.0|\n",
      "|2016.0|   4.0| 209.0|    DEN|20554.0|    1.0|20560.0|    1.0|\n",
      "|2016.0|   4.0| 115.0|    BOS|20555.0|    1.0|20556.0|    1.0|\n",
      "|2016.0|   4.0| 135.0|    WAS|20555.0|    1.0|20557.0|    1.0|\n",
      "|2016.0|   4.0| 135.0|    NYC|20555.0|    1.0|20559.0|    1.0|\n",
      "|2016.0|   4.0| 141.0|    CHI|20555.0|    1.0|20559.0|    1.0|\n",
      "|2016.0|   4.0| 115.0|    BOS|20556.0|    1.0|20557.0|    1.0|\n",
      "|2016.0|   4.0| 148.0|    CHI|20556.0|    1.0|20565.0|    1.0|\n",
      "|2016.0|   4.0| 131.0|    WAS|20568.0|    1.0|20572.0|    1.0|\n",
      "+------+------+------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract columns for immigration fact table\n",
    "df_clean_spark_i94 = df_clean_spark_i94.select([\"i94yr\", \"i94mon\", \"i94cit\", \"i94port\", \"arrdate\", \"i94mode\", \"depdate\", \"i94visa\"])\n",
    "\n",
    "# Write immigration fact table to parquet files partitioned by i94port\n",
    "df_clean_spark_i94.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/home/workspace//results/immigration.parquet\")\n",
    "\n",
    "# Show dataframe\n",
    "df_clean_spark_i94.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+--------------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|     City|             Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+---------+--------------------+--------+---------+-------+\n",
      "|1758-08-01|            13.167| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1819-03-01|             5.155| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1823-12-01|             5.776| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1854-05-01| 8.229000000000001| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1862-07-01|            10.848| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1875-12-01|             5.728| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1886-04-01|             5.282| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1918-12-01|6.9849999999999985| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1931-07-01|             12.85| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1948-10-01|             9.467| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1949-05-01|             8.417| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1991-12-01|             6.919| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|2004-09-01|            13.037| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|2005-10-01|            11.856| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|2009-02-01|             5.183| aberdeen|      United Kingdom|  57.05N|    1.48W|    ABE|\n",
      "|1866-01-01|            18.392|abu dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1869-06-01|            32.593|abu dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1891-02-01|            19.058|abu dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1907-06-01|             31.71|abu dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "|1918-03-01|             21.41|abu dhabi|United Arab Emirates|  24.92N|   54.98E|    MAA|\n",
      "+----------+------------------+---------+--------------------+--------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the dimension table \n",
    "df_clean_spark_temp = df_clean_spark_temp.select([\"dt\",\"AverageTemperature\",\"City\", \"Country\", \"Latitude\", \"Longitude\", \"i94port\"]).dropDuplicates()\n",
    "#df_clean_spark_temp = df_clean_spark_temp.select([\"AverageTemperature\", \"City\", \"Country\", \"Latitude\", \"Longitude\",\"i94port\"]).distinct()\n",
    "\n",
    "# Write dimension table to parquet files\n",
    "df_clean_spark_temp.write.parquet(\"/home/workspace//results/temperature.parquet\")\n",
    "# Dimension dataframe show\n",
    "df_clean_spark_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median_Age|Male_Population|Female_Population|Total_Population|Number_of_Veterans|Foreign-born|Average_Household_Size|State_Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create us-cities-demographics dimension table \n",
    "from pyspark.sql.functions import col\n",
    "df_clean_us_cities_demo=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "df_clean_us_cities_demo = df_clean_us_cities_demo.select(col(\"City\"),col(\"State\"),col(\"Median Age\").alias(\"Median_Age\"), col(\"Male Population\").alias(\"Male_Population\"),\n",
    "col(\"Female Population\").alias(\"Female_Population\"),\n",
    "col(\"Total Population\").alias(\"Total_Population\"),\n",
    "col(\"Number of Veterans\").alias(\"Number_of_Veterans\"),\n",
    "col(\"Foreign-born\"),\n",
    "col(\"Average Household Size\").alias(\"Average_Household_Size\"),\n",
    "col(\"State Code\").alias(\"State_Code\"),col(\"Race\"),col(\"Count\"))\n",
    "df_clean_us_cities_demo.write.parquet(\"/home/workspace//results/city_demographics.parquet\")\n",
    "\n",
    "df_clean_us_cities_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+-------+-------+-------+-------+----------+-------------------+-------------+--------------+--------+---------+\n",
      "|i94port| i94yr|i94mon|i94cit|arrdate|i94mode|depdate|i94visa|        dt| AverageTemperature|         City|       Country|Latitude|Longitude|\n",
      "+-------+------+------+------+-------+-------+-------+-------+----------+-------------------+-------------+--------------+--------+---------+\n",
      "|    HOU|2016.0|   4.0| 148.0|20552.0|    1.0|20558.0|    1.0|2007-09-01| 27.048000000000002|      houston| United States|  29.74N|   96.00W|\n",
      "|    LOS|2016.0|   4.0| 148.0|20560.0|    1.0|20569.0|    2.0|1937-10-01|             12.348|  los angeles|         Chile|  37.78S|   73.22W|\n",
      "|    LOS|2016.0|   4.0| 135.0|20558.0|    1.0|20566.0|    2.0|1870-09-01|             20.477|  los angeles| United States|  34.56N|  118.70W|\n",
      "|    MIA|2016.0|   4.0| 131.0|20550.0|    1.0|20556.0|    2.0|1831-04-01| 23.028000000000002|        miami| United States|  26.52N|   80.60W|\n",
      "|    MIA|2016.0|   4.0| 111.0|20559.0|    1.0|20561.0|    2.0|1928-07-01|             27.594|        miami| United States|  26.52N|   80.60W|\n",
      "|    NYC|2016.0|   4.0| 373.0|20567.0|    1.0|20568.0|    2.0|1858-12-01|             -1.068|     new york| United States|  40.99N|   74.56W|\n",
      "|    SFR|2016.0|   4.0| 135.0|20547.0|    1.0|20553.0|    1.0|1930-11-01|              12.34|san francisco| United States|  37.78N|  122.03W|\n",
      "|    WAS|2016.0|   4.0| 135.0|20555.0|    1.0|20557.0|    1.0|1987-05-01|             18.265|   washington| United States|  39.38N|   76.99W|\n",
      "|    NYC|2016.0|   4.0| 135.0|20551.0|    1.0|20563.0|    2.0|1751-09-01|             11.453|         york|United Kingdom|  53.84N|    1.36W|\n",
      "|    NYC|2016.0|   4.0| 135.0|20550.0|    1.0|20551.0|    1.0|1985-09-01| 13.834000000000001|         york|United Kingdom|  53.84N|    1.36W|\n",
      "|    BOS|2016.0|   4.0| 115.0|20559.0|    1.0|20561.0|    1.0|1778-11-01|0.40600000000000014|       boston| United States|  42.59N|   72.00W|\n",
      "|    LVG|2016.0|   4.0| 135.0|20558.0|    1.0|20579.0|    1.0|1927-10-01|             18.294|    las vegas| United States|  36.17N|  115.36W|\n",
      "|    LOS|2016.0|   4.0| 438.0|20552.0|    1.0|20572.0|    2.0|1954-04-01|             15.095|  los angeles| United States|  34.56N|  118.70W|\n",
      "|    MIA|2016.0|   4.0| 504.0|20546.0|    1.0|20548.0|    2.0|1813-09-01|             27.612|        miami| United States|  26.52N|   80.60W|\n",
      "|    NYC|2016.0|   4.0| 135.0|20550.0|    1.0|20551.0|    1.0|1907-06-01|             17.479|     new york| United States|  40.99N|   74.56W|\n",
      "|    NYC|2016.0|   4.0| 135.0|20572.0|    1.0|20573.0|    2.0|1933-10-01| 10.392000000000001|     new york| United States|  40.99N|   74.56W|\n",
      "|    WAS|2016.0|   4.0| 135.0|20567.0|    1.0|20572.0|    1.0|1921-05-01|             16.393|   washington| United States|  39.38N|   76.99W|\n",
      "|    NYC|2016.0|   4.0| 135.0|20566.0|    1.0|20567.0|    1.0|1836-03-01|              5.224|         york|United Kingdom|  53.84N|    1.36W|\n",
      "|    LOS|2016.0|   4.0| 135.0|20558.0|    1.0|20566.0|    1.0|1927-11-01|             25.967|      angeles|   Philippines|  15.27N|  120.83E|\n",
      "|    ATL|2016.0|   4.0| 148.0|20548.0|    1.0|20551.0|    1.0|1833-04-01|             14.908|      atlanta| United States|  34.56N|   83.68W|\n",
      "+-------+------+------+------+-------+-------+-------+-------+----------+-------------------+-------------+--------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_fact=df_clean_spark_i94.join(df_clean_spark_temp,['i94port'],how='inner').distinct()\n",
    "\n",
    "# Write immigration fact table to parquet files partitioned by i94port\n",
    "df_spark_fact.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"/home/workspace/results/result_fact.parquet\")\n",
    "\n",
    "# Show dataframe\n",
    "df_spark_fact.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation passed with 349057 with records for fact table\n",
      "Data validation passed with 89  records for i94 dimension table\n",
      "Data validation passed with 674065 records for temperature dimension table\n",
      "Data validation passed with 2891 records for city dempgraphic dimension table\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "result_df_spark_fact = df_spark_fact.count()\n",
    "if result_df_spark_fact != 0:\n",
    "    print(\"Data validation passed with {} with records for fact table\".format(result_df_spark_fact))\n",
    "else:\n",
    "    print(\"Data validation failed for fact table\")\n",
    "\n",
    "result_df_clean_spark_i94 = df_clean_spark_i94.count()\n",
    "if result_df_clean_spark_i94 != 0:\n",
    "    print(\"Data validation passed with {}  records for i94 dimension table\".format(result_df_clean_spark_i94))\n",
    "else:\n",
    "    print(\"Data quality failed  for i94 dimension table\")\n",
    "\n",
    "result_df_clean_spark_temp = df_clean_spark_temp.count()\n",
    "if result_df_clean_spark_temp != 0:\n",
    "    print(\"Data validation passed with {} records for temperature dimension table\".format(result_df_clean_spark_temp))\n",
    "else:\n",
    "    print(\"Data validation failed for temperature dimension table\")\n",
    "\n",
    "\n",
    "result_df_clean_us_cities_demo = df_clean_us_cities_demo.count()\n",
    "if result_df_clean_us_cities_demo != 0:\n",
    "    print(\"Data validation passed with {} records for city dempgraphic dimension table\".format(result_df_clean_us_cities_demo))\n",
    "else:\n",
    "    print(\"Data validation failed for city dempgraphic dimension table\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "Document stores data model is used where data is stored in Parquet file format with partitions. This will help in storage/ maintenance and extraction of data in Spark by different users.  \n",
    "\n",
    "The first dimension table will contain events from the I94 immigration data. The columns below will be extracted from the immigration dataframe:\n",
    "\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration\n",
    "\n",
    "The second dimension table will contain city temperature data. The columns below will be extracted from the temperature * dataframe:\n",
    "* i94port = 3 character code of destination city (mapped from immigration data during cleanup step)\n",
    "* AverageTemperature = average temperature\n",
    "* City = city name\n",
    "* Country = country name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude\n",
    "\n",
    "The third dimension table will contain city demographic data. The columns below will be extracted from the us-city-demo * dataframe:\n",
    "* City = city name\n",
    "* State = State where city is residing\n",
    "* Median_Age = Average age\n",
    "* Male_Population = Count of Male population\n",
    "* Female_Population = Count of Female population\n",
    "* Total_Population = Count of Total population\n",
    "* Number_of_Veterans = Number of Veterans\n",
    "* Foreign-born = Count of Foreign born\n",
    "* Average_Household_Size = Count of average household size\n",
    "* State_Code = State Code\n",
    "* Race = Race\n",
    "* Count = Count of race in city\n",
    "\n",
    "\n",
    "The fact table will contain information from the I94 immigration data joined with the city temperature data on i94port:\n",
    "\n",
    "* i94port = 3 character code of destination city\n",
    "* i94yr = 4 digit year\n",
    "* i94mon = numeric month\n",
    "* i94cit = 3 digit code of origin city\n",
    "* i94port = 3 character code of destination city\n",
    "* arrdate = arrival date\n",
    "* i94mode = 1 digit travel code\n",
    "* depdate = departure date\n",
    "* i94visa = reason for immigration\n",
    "* AverageTemperature = average temperature of destination city\n",
    "* City = city name\n",
    "* Country = country name\n",
    "* Latitude= latitude\n",
    "* Longitude = longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "Spark was chosen since it can easily handle multiple file formats (including SAS/ Parquet) containing large amounts of data. PySpark  and Spark dataframes including join operations was chosen to process the large input files and performed slicing and dicing to form additional tables.\n",
    "The final fact and dimension tables will be used by Data anlytics teams to generate reports and understand the traffic of immigrant movement in various cities\n",
    "and also help in track of the immigration population. It helps in understand various factors like mode of transport, what kind of major visa types are used.\n",
    "It also helps the analytics team to understand if temperature plays any\n",
    "vital role in the cities where people are migrating too. \n",
    "As we understand that the data volumes are so huge for every week across all the ports, loading data using Python pandas take\n",
    "huge time and loading this data into local memory and process will be of great challenge. Spark plays vital role in processing huge datasets.\n",
    "As we can see in our project it clearly shows with Spark architecture loading time for Pandas is lesser than Spark.\n",
    "When data increases this plays an important role.\n",
    "We have used Pyspark to process data in Spark instead of Scala as development time, community support comes to a big advantage\n",
    "using Pyspark.\n",
    "\n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "The data should be updated daily on the arrival of passengers by getting data from all valid  every airports.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " Cleaning and uploading the files to cloud S3 buckets(by partitioning properly across clusters) and perform incremental updates by using  multiple cluster model in AWS EMR / design Hadoop map reduce can handle very huge datasets. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " If the data needs to populate a dashboard daily by 7AM then we can use a scheduling tool such as Airflow or KRON or LUIGI to run the ETL pipeline.\n",
    " * The database needed to be accessed by 100+ people.\n",
    " Access can be provided to HUE(Web based query execution tool) or provide an impala engine connection and can run queries using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
